{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Davies Corpus: Acquisition Workflow**\nThis workflow ingests a locally downloaded Davies corpus (e.g., COHA, COCA) into a RocksDB database. Unlike Google Books ngrams which are downloaded on-the-fly, Davies corpora must be obtained separately and stored locally before running this notebook.",
   "id": "e5392ef8bda7b4d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "c941ee5a2f74fde8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:30:14.994705Z",
     "start_time": "2025-12-17T02:30:14.377316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from ngramkit.ngram_filter.lemmatizer import CachedSpacyLemmatizer\n",
    "from davieskit.davies_acquire import ingest_davies_corpus\n",
    "from davieskit.davies_filter import filter_davies_corpus, write_whitelist\n",
    "from ngramkit.utilities.peek import db_head, db_peek"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure\nHere we set basic parameters: the corpus name, local path to the downloaded corpus files, and database storage paths."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:30:15.140933Z",
     "start_time": "2025-12-17T02:30:14.998317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_name = 'COHA'\n",
    "db_path_stub = f'/scratch/edk202/NLP_corpora/{corpus_name}/'"
   ],
   "id": "111c835807fdb89",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Ingest Corpus into Database**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:32:42.227727Z",
     "start_time": "2025-12-17T02:30:24.008149Z"
    }
   },
   "source": [
    "combined_bigrams = {\n",
    "    \"working class\", \"working classes\",\n",
    "    \"middle class\", \"middle classes\",\n",
    "    \"lower class\", \"lower classes\",\n",
    "    \"upper class\", \"upper classes\",\n",
    "    \"human being\", \"human beings\"\n",
    "}\n",
    "\n",
    "ingest_davies_corpus(\n",
    "    db_path_stub = db_path_stub,\n",
    "    workers=24,\n",
    "    write_batch_size=500_000,\n",
    "    track_genre=False,\n",
    "    compact_after=True,\n",
    "    combined_bigrams=combined_bigrams\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHA CORPUS ACQUISITION\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-16 21:30:24\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Corpus path:          /scratch/edk202/NLP_corpora/COHA\n",
      "Text directory:       /scratch/edk202/NLP_corpora/COHA/text\n",
      "DB path:              /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "Text files found:     20\n",
      "Workers:              24\n",
      "Batch size:           500,000\n",
      "Genre tracking:       Disabled\n",
      "\n",
      "Processing Files\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████████████| 20/20 [01:53<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         1.71 GB\n",
      "Compaction completed in 0:00:21\n",
      "Size before:             1.71 GB\n",
      "Size after:              2.44 GB\n",
      "Space saved:             -741.19 MB (-42.3%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Files processed:          20/20\n",
      "Failed files:             0\n",
      "Total sentences written:  23,128,954\n",
      "Database path:            /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "\n",
      "End Time: 2025-12-16 21:32:40\n",
      "Total Runtime: 0:02:16.109666\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Filter Database**",
   "id": "6367a394a9898ca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:40:23.794418Z",
     "start_time": "2025-12-17T02:32:45.690798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filter_options = {\n",
    "    'stop_set': set(get_stop_words(\"english\")),\n",
    "    'lemma_gen': CachedSpacyLemmatizer()\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    \"working-class\", \"working-classes\",\n",
    "    \"middle-class\", \"middle-classes\",\n",
    "    \"lower-class\", \"lower-classes\",\n",
    "    \"upper-class\", \"upper-classes\",\n",
    "    \"human-being\", \"human-beings\"\n",
    "}\n",
    "\n",
    "filter_davies_corpus(\n",
    "    src_db_path=f'{db_path_stub}/{corpus_name}',\n",
    "    dst_db_path=f'{db_path_stub}/{corpus_name}_filtered',\n",
    "    workers=100,\n",
    "    batch_size=250_000,\n",
    "    track_genre=False,\n",
    "    create_whitelist=True,\n",
    "    whitelist_path=f'{db_path_stub}/{corpus_name}_whitelist.txt',\n",
    "    apply_whitelist=True,\n",
    "    whitelist_size=30_000,\n",
    "    whitelist_spell_check=True,\n",
    "    whitelist_year_range=(1900, 2020),\n",
    "    compact_after=True,\n",
    "    always_include=always_include_tokens,\n",
    "    **filter_options\n",
    ")"
   ],
   "id": "dcf8950dea7ee308",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHA CORPUS FILTERING\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-16 21:32:46\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "Destination DB:       /scratch/edk202/NLP_corpora/COHA/COHA_filtered\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Filter short:         True (min_len=3)\n",
      "Filter stops:         True\n",
      "Apply lemmas:         True\n",
      "Workers:              100\n",
      "Batch size:           250,000\n",
      "\n",
      "Processing Sentences\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Processed: 100%|███████████████████████████████████████████████████████| 89/89 [00:54<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Whitelist\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Whitelist path:          /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt\n",
      "Top N tokens:            30,000\n",
      "Year range:              1900-2020\n",
      "Spell check:             True\n",
      "\n",
      "Scanning database...\n",
      "Found 20,645,805 sentences in 413 batches\n",
      "Detecting years present in corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building token frequencies: 100%|████████████████████████████████████████████| 413/413 [03:33<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Years present in corpus within range: 11 years\n",
      "  Range: 1900 to 2000\n",
      "  Years: [1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "\n",
      "Filtering tokens by year coverage (must appear in all 11 years)...\n",
      "Tokens before year filter: 64,639\n",
      "Tokens after year filter:  23,845\n",
      "Tokens removed:            40,794\n",
      "\n",
      "Ranking 23,845 unique tokens...\n",
      "Selected top 30,000 tokens (+ 0 always_include)\n",
      "Writing whitelist to /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt...\n",
      "Whitelist written successfully: /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt\n",
      "\n",
      "Applying Whitelist\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Loading whitelist into memory...\n",
      "Loaded 23,845 tokens from whitelist\n",
      "\n",
      "Replacing non-whitelist tokens with <UNK>...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Processed: 100%|███████████████████████████████████████████████████████| 83/83 [00:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whitelist application complete!\n",
      "Sentences processed:      20,645,805\n",
      "Sentences modified:       719,178\n",
      "\n",
      "Post-Filter Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.39 GB\n",
      "Compaction completed in 0:00:18\n",
      "Size before:             2.39 GB\n",
      "Size after:              2.39 GB\n",
      "Space saved:             244.70 KB (0.0%)\n",
      "\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Sentences read:           22,172,210\n",
      "Writes accumulated:       20,802,803\n",
      "Sentences rejected:       1,369,407\n",
      "Retention rate:           93.8%\n",
      "Destination DB:           /scratch/edk202/NLP_corpora/COHA/COHA_filtered\n",
      "\n",
      "End Time: 2025-12-16 21:40:18\n",
      "Total Runtime: 0:07:32.701842\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": "## **Optional: Inspect Database**"
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": "### `db_head`: Show first N records"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T01:16:37.109240Z",
     "start_time": "2025-12-17T01:16:36.979086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}'\n",
    "\n",
    "db_head(str(db_path), n=15)"
   ],
   "id": "55493a0831eb239",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1810] &; Leon\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1810] &; Mad\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1810] &; Moth\n",
      "     Value: Total: 2 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1810] &; Stur\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1810] &; urg 'd him to the grave where sleeps His greatness Borodino 's field\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 6] Key:   [1810] 'The standard of good behavior for the continuance in office of the-judieial magistracy is certainly one of the most valuable of the modern improvements in the practice of government\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 7] Key:   [1810] 'm thine\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 8] Key:   [1810] 's defeated The whole rank 's slaughter 'd scarce a man escap 'd To tell their fate\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 9] Key:   [1810] 's my mother\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[10] Key:   [1810] 's royal spirit Would treat of peace\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[11] Key:   [1810] 've seen enough order the servants to shut all the doors and windows Oh\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[12] Key:   [1810] 've told you all my master 's names except two and one of them is Jake Flutercase\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[13] Key:   [1810] ( 1st Broker writes and Shaver advances on the stage\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[14] Key:   [1810] ( 2 ) and juries are to be the arbiters of justice\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[15] Key:   [1810] ( A boy at the door\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek`: Show records starting from a key",
   "id": "bf56e39cb59e741c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:42:54.273687Z",
     "start_time": "2025-12-17T02:42:54.103292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}'\n",
    "\n",
    "db_peek(db_path, start_key=\"[1980] I am a human-being\", n=5)\n"
   ],
   "id": "8fbde6180d828266",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007bc4920616d20612068756d616e2d6265696e67:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1980] I am a human-being too he pleaded on a note of despair\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1980] I am a hundred percent disbeliever in war\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1980] I am a layman a physician and a Catholic\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1980] I am a literature and history buff and therefore my examples are heavily weighted in those directions.\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1980] I am a little nervous too with everybody on and off the ship watching but this really\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek_prefix`: Records matching a prefix",
   "id": "d74c5d5e8f5b6cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:43:02.187070Z",
     "start_time": "2025-12-17T02:43:02.133926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}_filtered'\n",
    "\n",
    "db_peek(db_path, start_key=\"[1980] <UNK> <UNK> <UNK> human-being\", n=15)"
   ],
   "id": "2670fb8721bd9b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 key-value pairs starting from 000007bc3c554e4b3e203c554e4b3e203c554e4b3e2068756d616e2d6265696e67:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> movie <UNK> still available\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> <UNK> <UNK> right <UNK> <UNK> <UNK> life <UNK> future\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> <UNK> deserve well <UNK> <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> <UNK> plead <UNK> <UNK> note <UNK> despair\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> put <UNK> <UNK> slight danger <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 6] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> right\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 7] Key:   [1980] <UNK> <UNK> <UNK> human-being <UNK> say <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 8] Key:   [1980] <UNK> <UNK> <UNK> human-being whose <UNK> <UNK> vision <UNK> understand <UNK> <UNK> expand <UNK> <UNK> expand age <UNK> <UNK> <UNK> play <UNK> central role <UNK> <UNK> appreciate <UNK> tangle drive <UNK> unlike <UNK> <UNK> <UNK> motivate <UNK> two young man\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 9] Key:   [1980] <UNK> <UNK> <UNK> human-beings <UNK> <UNK> <UNK> world tend <UNK> order <UNK> <UNK> <UNK> environment <UNK> remarkably similar way <UNK> <UNK> implicit recourse <UNK> classificatory principle <UNK> general <UNK> adaptive <UNK> <UNK> appear natural proclivity <UNK> <UNK> human mind\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[10] Key:   [1980] <UNK> <UNK> <UNK> human-beings <UNK> know\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[11] Key:   [1980] <UNK> <UNK> <UNK> human-beings can <UNK> <UNK> obstruction\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[12] Key:   [1980] <UNK> <UNK> <UNK> human-beings grow <UNK> small creature form within <UNK> body <UNK> <UNK> father\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[13] Key:   [1980] <UNK> <UNK> <UNK> human-beings spend <UNK> equivalent <UNK> air fare <UNK> distant place <UNK> get high <UNK> <UNK> minute\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[14] Key:   [1980] <UNK> <UNK> <UNK> humane man <UNK> contemporary account <UNK> <UNK> behavior <UNK> <UNK> analyst indicate <UNK> <UNK> sometimes show quite human emotion <UNK> <UNK> patient occasionally reveal personal matter like <UNK> accident <UNK> <UNK> son <UNK> <UNK> even report <UNK> thump <UNK> table <UNK> hammer home <UNK> point\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[15] Key:   [1980] <UNK> <UNK> <UNK> humanist <UNK> <UNK> <UNK> beech <UNK> <UNK> <UNK> <UNK> group <UNK> <UNK> <UNK> case <UNK> <UNK> state\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "74892a369be1afc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
