{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Davies Corpus: Acquisition Workflow**\nThis workflow ingests a locally downloaded Davies corpus (e.g., COHA, COCA) into a RocksDB database. Unlike Google Books ngrams which are downloaded on-the-fly, Davies corpora must be obtained separately and stored locally before running this notebook.",
   "id": "e5392ef8bda7b4d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "c941ee5a2f74fde8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:56:18.009500Z",
     "start_time": "2025-12-09T20:56:17.120153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from ngramkit.ngram_filter.lemmatizer import CachedSpacyLemmatizer\n",
    "from davieskit.davies_acquire import ingest_davies_corpus\n",
    "from davieskit.davies_filter import filter_davies_corpus, FilterConfig\n",
    "from ngramkit.utilities.peek import db_head, db_peek"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure\nHere we set basic parameters: the corpus name, local path to the downloaded corpus files, and database storage paths."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:41:08.596952Z",
     "start_time": "2025-12-09T20:41:08.591207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_name = 'COHA'\n",
    "db_path_stub = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}'"
   ],
   "id": "111c835807fdb89",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Ingest Corpus into Database**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:46:28.644644Z",
     "start_time": "2025-12-09T20:41:16.719071Z"
    }
   },
   "source": [
    "ingest_davies_corpus(\n",
    "    db_path_stub = db_path_stub,\n",
    "    workers=24,\n",
    "    write_batch_size=500_000,\n",
    "    compact_after_ingest=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHA CORPUS ACQUISITION\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-09 15:41:16\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Corpus path:          /scratch/edk202/NLP_corpora/COHA\n",
      "Text directory:       /scratch/edk202/NLP_corpora/COHA/text\n",
      "DB path:              /scratch/edk202/NLP_corpora/COHA/db\n",
      "Text files found:     20\n",
      "Workers:              24\n",
      "Batch size:           500,000\n",
      "\n",
      "Processing Files\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Processed 5/20 files (2,330,050 sentences)\n",
      "  Processed 10/20 files (7,204,389 sentences)\n",
      "  Processed 15/20 files (14,626,031 sentences)\n",
      "  Processed 20/20 files (23,168,949 sentences)\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         1.74 GB\n",
      "Compaction completed in 0:00:25\n",
      "Size before:             1.74 GB\n",
      "Size after:              2.48 GB\n",
      "Space saved:             -750.38 MB (-42.0%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Files processed:          20/20\n",
      "Failed files:             0\n",
      "Total sentences written:  23,168,949\n",
      "Database path:            /scratch/edk202/NLP_corpora/COHA/db\n",
      "\n",
      "End Time: 2025-12-09 15:46:28\n",
      "Total Runtime: 0:05:11.899341\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Filter Database**",
   "id": "6367a394a9898ca9"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-09T20:57:23.914446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filter_options = {\n",
    "    'stop_set': set(get_stop_words(\"english\")),\n",
    "    'lemma_gen': CachedSpacyLemmatizer()\n",
    "}\n",
    "\n",
    "filter_davies_corpus(\n",
    "    src_db_path=f'{db_path_stub}/db',\n",
    "    dst_db_path=f'{db_path_stub}/db/{corpus_name}_filtered',\n",
    "    **filter_options\n",
    ")"
   ],
   "id": "dcf8950dea7ee308",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVIES CORPUS FILTERING\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-09 15:57:24\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/COHA/db\n",
      "Destination DB:       /scratch/edk202/NLP_corpora/COHA/db/COHA_filtered\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Filter short:         True (min_len=3)\n",
      "Filter stops:         True\n",
      "Apply lemmas:         True\n",
      "Batch size:           100,000\n",
      "\n",
      "Processing Sentences\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Processed 500,000 sentences (469,044 unique seen, 29,178 rejected)\n",
      "  Processed 1,000,000 sentences (942,508 unique seen, 53,932 rejected)\n",
      "  Processed 1,500,000 sentences (1,421,916 unique seen, 72,716 rejected)\n",
      "  Processed 2,000,000 sentences (1,892,790 unique seen, 99,805 rejected)\n",
      "  Processed 2,500,000 sentences (2,360,727 unique seen, 129,581 rejected)\n",
      "  Processed 3,000,000 sentences (2,833,345 unique seen, 154,503 rejected)\n",
      "  Processed 3,500,000 sentences (3,300,703 unique seen, 184,344 rejected)\n",
      "  Processed 4,000,000 sentences (3,765,211 unique seen, 216,922 rejected)\n",
      "  Processed 4,500,000 sentences (4,238,492 unique seen, 241,068 rejected)\n",
      "  Processed 5,000,000 sentences (4,703,859 unique seen, 273,194 rejected)\n",
      "  Processed 5,500,000 sentences (5,177,991 unique seen, 296,807 rejected)\n",
      "  Processed 6,000,000 sentences (5,645,294 unique seen, 327,235 rejected)\n",
      "  Processed 6,500,000 sentences (6,117,516 unique seen, 352,550 rejected)\n",
      "  Processed 7,000,000 sentences (6,580,384 unique seen, 386,635 rejected)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": "## **Optional: Inspect Database**"
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": "### `db_head`: Show first N records"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:46:44.165868Z",
     "start_time": "2025-12-09T20:46:43.999260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/db'\n",
    "\n",
    "db_head(str(db_path), n=5)"
   ],
   "id": "55493a0831eb239",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1810] &; Leon\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1810] &; Mad\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1810] &; Moth\n",
      "     Value: Total: 2 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1810] &; Stur\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1810] 'The standard of good behavior for the continuance in office of the-judieial magistracy is certainly one of the most valuable of the modern improvements in the practice of government\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek`: Show records starting from a key",
   "id": "bf56e39cb59e741c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:47:04.238758Z",
     "start_time": "2025-12-09T20:47:04.192784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/db'\n",
    "\n",
    "db_peek(db_path, start_key=\"[1990] The horror\", n=5)\n"
   ],
   "id": "8fbde6180d828266",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007c654686520686f72726f72:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1990] The horror\n",
      "     Value: Total: 4 occurrences in 2 volumes (1990-1990, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1990] The horror and the shame were like two vicious heavy blows\n",
      "     Value: Total: 1 occurrences in 1 volumes (1990-1990, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1990] The horror meant he still was n't sure\n",
      "     Value: Total: 1 occurrences in 1 volumes (1990-1990, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1990] The horror novel I read on the Bullet Train now rides low just inside my jacket pocket and Lew Spencer the agent who receives me at the door sees it and smiles\n",
      "     Value: Total: 1 occurrences in 1 volumes (1990-1990, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1990] The horror of being institutionalized has kept my daughter faithful to her medications\n",
      "     Value: Total: 1 occurrences in 1 volumes (1990-1990, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek_prefix`: Records matching a prefix",
   "id": "d74c5d5e8f5b6cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:30:17.503606Z",
     "start_time": "2025-12-09T20:30:17.471518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/db'\n",
    "\n",
    "db_peek(db_path, start_key=\"[1980] nuclear weapons\", n=5)"
   ],
   "id": "2670fb8721bd9b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007bc6e75636c65617220776561706f6e73:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1980] nuclear weapons and the payment of taxes\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1980] nuclear weapons have no military purpose other than to prevent a nuclear attack on the homeland of their possessors\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1980] nuclear weapons in Spain\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1980] nuclear weapons policy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1980] nuclear weapons stored in that country and in Turkey\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "74892a369be1afc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
