{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T04:20:09.818717Z",
     "start_time": "2025-11-20T04:20:08.727068Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2\n\nfrom ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\nfrom ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\nfrom ngramkit.ngram_pivot.pipeline import build_pivoted_db\nfrom ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix",
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure"
  },
  {
   "cell_type": "code",
   "id": "config_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T04:20:10.079936Z",
     "start_time": "2025-11-20T04:20:09.822298Z"
    }
   },
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "size = 5"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Step 1: Download and Ingest**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T02:28:55.526113Z",
     "start_time": "2025-11-19T22:39:24.983217Z"
    }
   },
   "source": [
    "combined_bigrams_download = {\"lower class\", \"working class\", \"middle class\", \"upper class\"}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=98,\n",
    "    overwrite_db=True,\n",
    "    workers=80,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-19 17:39:24\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "File range:           0 to 19422\n",
      "Total files:          19423\n",
      "Files to get:         19423\n",
      "Skipping:             0\n",
      "Download workers:     80\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████| 19423/19423 [3:31:23<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.21 TB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compaction failed: IO error: While pwrite to file at offset 353370112: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/003740.sst: Disk quota exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compaction failed: IO error: While pwrite to file at offset 353370112: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/003740.sst: Disk quota exceeded\n",
      "Database is still usable, but may not be optimally compacted.\n",
      "Warning: Database flush failed for /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db: IO error: While pwrite to file at offset 353370112: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/003740.sst: Disk quota exceeded\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       19423\n",
      "Failed files:                0\n",
      "Total entries written:       2,480,130,227\n",
      "Write batches flushed:       6456\n",
      "Uncompressed data processed: 28.44 TB\n",
      "Processing throughput:       2165.36 MB/sec\n",
      "\n",
      "End Time: 2025-11-19 21:28:55.445684\n",
      "Total Runtime: 3:49:30.456982\n",
      "Time per file: 0:00:00.708977\n",
      "Files per hour: 5077.7\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T04:16:58.431456Z",
     "start_time": "2025-11-20T03:05:42.969057Z"
    }
   },
   "source": [
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\"lower-class\", \"working-class\", \"middle-class\", \"upper-class\"}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    **whitelist_options\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-19 22:05:42\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 15,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    17.27M        72.1%         36/36       539·36·25      554.0k/s        31s      \n",
      "    98.81M        79.1%         36/36       499·36·65     1615.0k/s       1m01s     \n",
      "   173.97M        78.4%         36/36       466·36·98     1908.4k/s       1m31s     \n",
      "   260.97M        79.2%         36/36       441·36·123    2153.6k/s       2m01s     \n",
      "   352.01M        80.5%         36/36       422·36·142    2328.3k/s       2m31s     \n",
      "   445.96M        81.0%         36/36       401·36·163    2461.3k/s       3m01s     \n",
      "   535.78M        81.2%         36/36       384·36·180    2537.2k/s       3m31s     \n",
      "   631.33M        81.6%         36/36       360·36·204    2617.7k/s       4m01s     \n",
      "   720.76M        81.8%         36/36       339·36·225    2657.8k/s       4m31s     \n",
      "   823.23M        82.0%         36/36       324·36·240    2733.7k/s       5m01s     \n",
      "   912.72M        81.9%         36/36       300·36·264    2755.8k/s       5m31s     \n",
      "    1.01B         82.1%         36/36       277·36·287    2798.5k/s       6m01s     \n",
      "    1.11B         82.2%         36/36       253·36·311    2829.2k/s       6m31s     \n",
      "    1.21B         82.0%         36/36       237·36·327    2863.9k/s       7m01s     \n",
      "    1.30B         82.3%         36/36       221·36·343    2880.0k/s       7m31s     \n",
      "    1.40B         82.4%         36/36       210·36·354    2915.4k/s       8m01s     \n",
      "    1.50B         82.2%         36/36       190·36·374    2939.3k/s       8m31s     \n",
      "    1.60B         82.1%         36/36       169·36·395    2952.4k/s       9m01s     \n",
      "    1.69B         82.0%         36/36       137·36·427    2961.6k/s       9m31s     \n",
      "    1.79B         82.0%         35/36       109·35·456    2979.1k/s       10m01s    \n",
      "    1.87B         82.0%         36/36       79·36·485     2958.3k/s       10m31s    \n",
      "    1.96B         81.9%         36/36       54·36·510     2968.6k/s       11m01s    \n",
      "    2.05B         82.0%         36/36       28·36·536     2966.8k/s       11m31s    \n",
      "    2.16B         82.1%         36/36        9·36·555     2988.9k/s       12m01s    \n",
      "    2.25B         82.3%         32/36        0·32·568     3000.3k/s       12m31s    \n",
      "    2.34B         82.3%         24/36        0·24·576     3001.6k/s       13m01s    \n",
      "    2.41B         82.5%         15/36        0·15·585     2967.3k/s       13m31s    \n",
      "    2.45B         82.5%         10/36        0·10·590     2906.8k/s       14m01s    \n",
      "    2.47B         82.6%          6/36        0·6·594      2835.7k/s       14m31s    \n",
      "    2.48B         82.6%          1/36        0·1·599      2752.0k/s       15m01s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         82.6%          0/36        0·0·600      2693.7k/s       15m20s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [44:36<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 454,492,241 items in 2676.8s (169,789 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         443.99 GB\n",
      "Compaction completed in 0:11:03\n",
      "Size before:             443.99 GB\n",
      "Size after:              277.36 GB\n",
      "Space saved:             166.62 GB (37.5%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 405,318,041 (estimated)                                                                   │\n",
      "│ Size: 277.36 GB                                                                                  │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "            ## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:06:15.391993Z",
     "start_time": "2025-11-20T04:20:15.572811Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-19 23:20:15\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     7.05M            0.0x          560·40·0        233.2k/s          30s       \n",
      "     34.16M           0.0x          560·40·0        567.3k/s         1m00s      \n",
      "     34.16M           0.0x          560·40·0        378.7k/s         1m30s      \n",
      "     56.38M          38.6x         541·40·19        469.1k/s         2m00s      \n",
      "     97.97M          39.3x         522·40·38        652.2k/s         2m30s      \n",
      "    106.39M          38.5x         520·40·40        590.3k/s         3m00s      \n",
      "    120.40M          45.9x         510·40·50        572.7k/s         3m30s      \n",
      "    159.17M          46.7x         488·40·72        662.5k/s         4m00s      \n",
      "    175.12M          47.5x         474·40·86        648.1k/s         4m30s      \n",
      "    200.44M          50.4x         450·40·110       667.6k/s         5m00s      \n",
      "    230.31M          50.7x         424·40·136       697.4k/s         5m30s      \n",
      "    259.51M          49.6x         396·40·164       720.4k/s         6m00s      \n",
      "    288.27M          50.6x         348·40·212       738.7k/s         6m30s      \n",
      "    317.33M          50.2x         291·39·270       755.1k/s         7m00s      \n",
      "    340.74M          48.6x         261·40·299       756.8k/s         7m30s      \n",
      "    371.83M          48.7x         224·40·336       774.3k/s         8m00s      \n",
      "    396.34M          49.0x         195·40·365       776.8k/s         8m30s      \n",
      "    421.86M          48.9x         166·40·394       780.9k/s         9m00s      \n",
      "    451.36M          48.7x         127·40·433       791.5k/s         9m30s      \n",
      "    477.66M          48.7x         36·40·524        795.8k/s         10m00s     \n",
      "    491.96M          49.1x          0·2·598         780.6k/s         10m30s     \n",
      "    492.52M          49.1x          0·1·599         746.0k/s         11m00s     \n",
      "    492.52M          49.1x          0·1·599         713.6k/s         11m30s     \n",
      "    492.59M          49.2x          0·1·599         683.9k/s         12m00s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    493.54M          49.3x          0·0·600         676.2k/s         12m09s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:35<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         175.11 GB\n",
      "Compaction completed in 0:33:01\n",
      "Size before:             175.11 GB\n",
      "Size after:              429.77 GB\n",
      "Space saved:             -254.66 GB (-145.4%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 12,155,533,738 (estimated)                                                                │\n",
      "│ Size: 429.77 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:06:22.801148Z",
     "start_time": "2025-11-20T05:06:21.357Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> convenient\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> <UNK> eng\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> atomic energy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> convenient form\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> convenient one\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:06:28.969656Z",
     "start_time": "2025-11-20T05:06:27.601920Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e3776f726b696e672d636c617373203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] working-class <UNK> <UNK> ability\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 2] Key:   [2019] working-class <UNK> <UNK> able\n",
      "     Value: 28 occurrences in 27 documents\n",
      "\n",
      "[ 3] Key:   [2019] working-class <UNK> <UNK> abolition\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n",
      "[ 4] Key:   [2019] working-class <UNK> <UNK> absence\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 5] Key:   [2019] working-class <UNK> <UNK> achievement\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:08:55.552233Z",
     "start_time": "2025-11-20T05:08:54.124715Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[1900] <UNK> lower-class\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 0000076c3c554e4b3e206c6f7765722d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1900] <UNK> lower-class <UNK> <UNK>\n",
      "     Value: 1,323 occurrences in 1,286 documents\n",
      "\n",
      "[ 2] Key:   [1900] <UNK> lower-class <UNK> able\n",
      "     Value: 7 occurrences in 7 documents\n",
      "\n",
      "[ 3] Key:   [1900] <UNK> lower-class <UNK> although\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1900] <UNK> lower-class <UNK> case\n",
      "     Value: 14 occurrences in 14 documents\n",
      "\n",
      "[ 5] Key:   [1900] <UNK> lower-class <UNK> caste\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
