{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:41:01.560578Z",
     "start_time": "2025-12-09T20:41:00.867334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramkit.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T20:41:01.739932Z",
     "start_time": "2025-12-09T20:41:01.564189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ],
   "id": "d5328f85c059eda4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Step 1: Download and Ingest**",
   "id": "2996606b00401532"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {},
   "source": [
    "combined_bigrams_download = {\"lower class\", \"working class\", \"middle class\", \"upper class\", \"human being\"}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=98,\n",
    "    overwrite_db=False,\n",
    "    workers=80,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T01:41:48.168330Z",
     "start_time": "2025-12-04T00:26:35.571721Z"
    }
   },
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\"lower-class\", \"working-class\", \"middle-class\", \"upper-class\", \"human-being\"}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-03 19:26:35\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "   255.91K        68.0%         36/36        558·36·6       8.4k/s         30s      \n",
      "    10.98M        76.8%         36/36       536·36·28      181.6k/s       1m00s     \n",
      "    58.02M        78.4%         36/36       508·36·56      641.5k/s       1m30s     \n",
      "   120.44M        80.6%         36/36       477·36·87     1000.3k/s       2m00s     \n",
      "   210.65M        81.1%         36/36       463·36·101    1400.3k/s       2m30s     \n",
      "   302.26M        81.9%         36/36       451·36·113    1675.2k/s       3m00s     \n",
      "   393.68M        82.9%         36/36       432·36·132    1870.7k/s       3m30s     \n",
      "   478.88M        83.4%         36/36       408·36·156    1991.9k/s       4m00s     \n",
      "   562.04M        83.9%         36/36       383·36·181    2078.4k/s       4m30s     \n",
      "   648.69M        83.6%         36/36       369·36·195    2159.3k/s       5m00s     \n",
      "   729.97M        83.3%         36/36       347·36·217    2209.1k/s       5m30s     \n",
      "   813.55M        83.0%         36/36       326·35·239    2257.1k/s       6m00s     \n",
      "   897.30M        83.1%         36/36       308·36·256    2298.2k/s       6m30s     \n",
      "   970.70M        83.5%         36/36       282·36·282    2308.7k/s       7m00s     \n",
      "    1.05B         83.6%         36/36       262·36·302    2338.5k/s       7m30s     \n",
      "    1.14B         83.8%         36/36       238·36·326    2364.3k/s       8m00s     \n",
      "    1.23B         83.7%         36/36       219·36·345    2406.2k/s       8m30s     \n",
      "    1.31B         83.7%         36/36       199·36·365    2430.7k/s       9m00s     \n",
      "    1.41B         83.8%         36/36       190·35·375    2469.7k/s       9m30s     \n",
      "    1.51B         83.9%         36/36       174·36·390    2509.4k/s       10m00s    \n",
      "    1.60B         83.9%         36/36       156·36·408    2540.0k/s       10m30s    \n",
      "    1.69B         83.9%         36/36       134·36·430    2554.1k/s       11m00s    \n",
      "    1.77B         83.9%         36/36       109·36·455    2559.1k/s       11m30s    \n",
      "    1.85B         83.9%         36/36       82·36·482     2572.2k/s       12m00s    \n",
      "    1.94B         83.8%         36/36       65·36·499     2578.7k/s       12m30s    \n",
      "    2.02B         83.7%         36/36       43·36·521     2592.0k/s       13m00s    \n",
      "    2.11B         83.6%         35/36       28·35·537     2606.6k/s       13m30s    \n",
      "    2.20B         83.6%         36/36        9·36·555     2613.4k/s       14m00s    \n",
      "    2.28B         83.7%         25/36        0·25·575     2620.3k/s       14m30s    \n",
      "    2.35B         83.8%         22/36        0·22·578     2608.3k/s       15m00s    \n",
      "    2.40B         84.0%         15/36        0·15·585     2583.0k/s       15m30s    \n",
      "    2.44B         84.0%          9/36        0·9·591      2545.6k/s       16m00s    \n",
      "    2.47B         84.0%          5/36        0·5·595      2490.4k/s       16m30s    \n",
      "    2.47B         84.1%          1/36        0·1·599      2425.1k/s       17m00s    \n",
      "    2.48B         84.1%          1/36        0·1·599      2359.6k/s       17m30s    \n",
      "    2.48B         84.1%          1/36        0·1·599      2295.5k/s       18m00s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         84.1%          0/36        0·0·600      2271.0k/s       18m12s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [45:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 500,277,946 items in 2740.2s (182,571 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         488.21 GB\n",
      "Compaction completed in 0:11:15\n",
      "Size before:             488.21 GB\n",
      "Size after:              308.16 GB\n",
      "Space saved:             180.05 GB (36.9%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 450,803,487 (estimated)                                                                   │\n",
      "│ Size: 308.16 GB                                                                                  │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T03:26:19.967935Z",
     "start_time": "2025-12-04T03:11:01.649520Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"resume\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-03 22:11:01\n",
      "Mode:       RESUME\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Resuming existing work units\n",
      "Resuming: 0 completed, 0 processing, 0 pending\n",
      "\n",
      "Phase 2: Processing 0 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       0              0.0x           0·0·0            0/s             14s       \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "       0              0.0x           0·0·0            0/s             17s       \n",
      "No shards to ingest (all already ingested)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         478.96 GB\n",
      "Compaction completed in 0:14:52\n",
      "Size before:             478.96 GB\n",
      "Size after:              478.96 GB\n",
      "Space saved:             12.70 KB (0.0%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 13,499,217,693 (estimated)                                                                │\n",
      "│ Size: 478.96 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T03:26:40.318253Z",
     "start_time": "2025-12-04T03:26:38.410019Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> convenient\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> <UNK> eng\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> atomic energy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> convenient form\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> convenient one\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ],
   "id": "c3519e6ee50c4109"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `db_peek_prefix`: Records matching a prefix",
   "id": "f6c63e70a0ce04fb"
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T03:27:49.890765Z",
     "start_time": "2025-12-04T03:27:47.950540Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2006] <UNK> working-class\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007d63c554e4b3e20776f726b696e672d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2006] <UNK> working-class <UNK>\n",
      "     Value: 12 occurrences in 12 documents\n",
      "\n",
      "[ 2] Key:   [2006] <UNK> working-class <UNK> <UNK>\n",
      "     Value: 27,473 occurrences in 25,360 documents\n",
      "\n",
      "[ 3] Key:   [2006] <UNK> working-class <UNK> able\n",
      "     Value: 19 occurrences in 19 documents\n",
      "\n",
      "[ 4] Key:   [2006] <UNK> working-class <UNK> absolutely\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n",
      "[ 5] Key:   [2006] <UNK> working-class <UNK> access\n",
      "     Value: 6 occurrences in 6 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
