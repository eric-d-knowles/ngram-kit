{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T03:15:15.674744Z",
     "start_time": "2025-11-24T03:15:15.022154Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2\n\nfrom ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\nfrom ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\nfrom ngramkit.ngram_pivot.pipeline import build_pivoted_db\nfrom ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix",
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure"
  },
  {
   "cell_type": "code",
   "id": "config_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T03:15:15.845549Z",
     "start_time": "2025-11-24T03:15:15.687470Z"
    }
   },
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "size = 5"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Step 1: Download and Ingest**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T02:22:26.463849Z",
     "start_time": "2025-11-24T01:40:01.275708Z"
    }
   },
   "source": [
    "combined_bigrams_download = {\"lower class\", \"working class\", \"middle class\", \"upper class\", \"blue collar\", \"white collar\", \"african american\", \"african americans\", \"european american\", \"european americans\", \"white people\", \"white person\", \"white americans\", \"black american\", \"black americans\", \"black person\", \"black people\", \"human being\"}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=98,\n",
    "    overwrite_db=False,\n",
    "    workers=80,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-23 20:40:01\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "File range:           0 to 19422\n",
      "Total files:          19423\n",
      "Files to get:         0\n",
      "Skipping:             19423\n",
      "Download workers:     80\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         False\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed:   0%|                                                               | 0/0 [00:00<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.21 TB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compaction completed in 0:42:07\n",
      "Size before:             2.21 TB\n",
      "Size after:              2.21 TB\n",
      "Space saved:             -143.08 KB (-0.0%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       0\n",
      "Failed files:                0\n",
      "Total entries written:       0\n",
      "Write batches flushed:       0\n",
      "Uncompressed data processed: 0.00 B\n",
      "Processing throughput:       0.00 MB/sec\n",
      "\n",
      "End Time: 2025-11-23 21:22:26.459660\n",
      "Total Runtime: 0:42:25.150204\n",
      "Time per file: 0:00:00\n",
      "Files per hour: 0.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T04:24:52.670931Z",
     "start_time": "2025-11-24T03:15:22.348391Z"
    }
   },
   "source": [
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\"lower-class\", \"working-class\", \"middle-class\", \"upper-class\", \"blue-collar\", \"white-collar\", \"african-american\", \"african-americans\", \"european-american\", \"european-americans\", \"white-people\", \"white-person\", \"white-americans\", \"black-american\", \"black-americans\", \"black-person\", \"black-people\", \"human-being\"}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    **whitelist_options\n",
    ");"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-23 22:15:22\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 15,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    25.98M        79.4%         36/36       540·36·24      853.7k/s        30s      \n",
      "   103.15M        81.6%         36/36       509·36·55     1708.0k/s       1m00s     \n",
      "   179.79M        81.8%         36/36       480·36·84     1988.2k/s       1m30s     \n",
      "   279.97M        81.3%         36/36       467·36·97     2325.5k/s       2m00s     \n",
      "   380.89M        81.6%         36/36       456·36·108    2532.3k/s       2m30s     \n",
      "   475.63M        82.4%         36/36       439·36·125    2636.2k/s       3m00s     \n",
      "   562.54M        82.8%         36/36       415·36·149    2673.8k/s       3m30s     \n",
      "   651.12M        82.8%         36/36       391·36·173    2708.4k/s       4m00s     \n",
      "   739.85M        82.6%         36/36       373·36·191    2736.0k/s       4m30s     \n",
      "   839.26M        82.2%         36/36       357·36·207    2793.5k/s       5m00s     \n",
      "   925.93M        82.3%         36/36       337·36·227    2802.4k/s       5m30s     \n",
      "    1.02B         82.5%         36/36       315·36·249    2826.3k/s       6m00s     \n",
      "    1.10B         82.4%         36/36       281·36·283    2815.1k/s       6m30s     \n",
      "    1.19B         82.3%         36/36       255·36·309    2820.5k/s       7m00s     \n",
      "    1.27B         82.3%         36/36       232·36·332    2827.4k/s       7m30s     \n",
      "    1.36B         82.3%         36/36       206·36·358    2824.4k/s       8m00s     \n",
      "    1.44B         82.4%         36/36       184·36·380    2826.4k/s       8m30s     \n",
      "    1.54B         82.5%         36/36       169·36·395    2851.3k/s       9m00s     \n",
      "    1.64B         82.7%         36/36       147·36·417    2866.8k/s       9m30s     \n",
      "    1.72B         82.7%         36/36       127·36·437    2868.2k/s       10m00s    \n",
      "    1.82B         82.8%         36/36       112·36·452    2883.1k/s       10m30s    \n",
      "    1.91B         82.8%         36/36       88·36·476     2890.4k/s       11m00s    \n",
      "    2.00B         82.7%         36/36       70·36·494     2894.0k/s       11m30s    \n",
      "    2.09B         82.6%         36/36       52·36·512     2895.4k/s       12m00s    \n",
      "    2.18B         82.6%         36/36       30·36·534     2899.8k/s       12m30s    \n",
      "    2.27B         82.7%         36/36        1·36·563     2902.9k/s       13m00s    \n",
      "    2.35B         82.6%         21/36        0·21·579     2894.6k/s       13m30s    \n",
      "    2.40B         82.5%         18/36        0·18·582     2861.7k/s       14m00s    \n",
      "    2.44B         82.5%          8/36        0·8·592      2808.8k/s       14m30s    \n",
      "    2.47B         82.6%          5/36        0·5·595      2740.2k/s       15m00s    \n",
      "    2.48B         82.6%          2/36        0·2·598      2664.2k/s       15m30s    \n",
      "    2.48B         82.6%          0/36        0·0·600      2582.3k/s       16m00s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         82.6%          0/36        0·0·600      2580.5k/s       16m01s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [42:53<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 454,493,730 items in 2573.3s (176,622 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         443.99 GB\n",
      "Compaction completed in 0:10:27\n",
      "Size before:             443.99 GB\n",
      "Size after:              277.36 GB\n",
      "Space saved:             166.62 GB (37.5%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 405,318,801 (estimated)                                                                   │\n",
      "│ Size: 277.36 GB                                                                                  │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-24T04:38:11.620775Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-23 23:38:11\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:06:22.801148Z",
     "start_time": "2025-11-20T05:06:21.357Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> convenient\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> <UNK> eng\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> atomic energy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> convenient form\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> convenient one\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:06:28.969656Z",
     "start_time": "2025-11-20T05:06:27.601920Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e3776f726b696e672d636c617373203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] working-class <UNK> <UNK> ability\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 2] Key:   [2019] working-class <UNK> <UNK> able\n",
      "     Value: 28 occurrences in 27 documents\n",
      "\n",
      "[ 3] Key:   [2019] working-class <UNK> <UNK> abolition\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n",
      "[ 4] Key:   [2019] working-class <UNK> <UNK> absence\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 5] Key:   [2019] working-class <UNK> <UNK> achievement\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:08:55.552233Z",
     "start_time": "2025-11-20T05:08:54.124715Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[1900] <UNK> lower-class\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 0000076c3c554e4b3e206c6f7765722d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1900] <UNK> lower-class <UNK> <UNK>\n",
      "     Value: 1,323 occurrences in 1,286 documents\n",
      "\n",
      "[ 2] Key:   [1900] <UNK> lower-class <UNK> able\n",
      "     Value: 7 occurrences in 7 documents\n",
      "\n",
      "[ 3] Key:   [1900] <UNK> lower-class <UNK> although\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1900] <UNK> lower-class <UNK> case\n",
      "     Value: 14 occurrences in 14 documents\n",
      "\n",
      "[ 5] Key:   [1900] <UNK> lower-class <UNK> caste\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
